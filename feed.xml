<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://raymond112514.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://raymond112514.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-05T16:39:14+00:00</updated><id>https://raymond112514.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Introduction to Statistical Learning Theory</title><link href="https://raymond112514.github.io/blog/2025/introduction-to-statistical-learning-theory/" rel="alternate" type="text/html" title="Introduction to Statistical Learning Theory"/><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><id>https://raymond112514.github.io/blog/2025/introduction-to-statistical-learning-theory</id><content type="html" xml:base="https://raymond112514.github.io/blog/2025/introduction-to-statistical-learning-theory/"><![CDATA[<style>.post{line-height:2.0}</style> <p>This blog series is meant to serve as my course notes for STAT 210B.</p> <h3 id="empirical-risk-minimization">Empirical Risk Minimization</h3> <p>In a standard supervised learning setup, we are given two spaces, $\mathcal{X}$ and $\mathcal{Y}$, and the goal is to learn a function $f \in \mathcal{F}$ that predicts $Y \in \mathcal{Y}$ given $X \in \mathcal{X}$. This is typically done by solving the following optimization problem:</p> \[f^*=\text{argmin}_{f\in \mathcal{F}} \underbrace{\mathbb{E}_{X, Y\sim \mathcal{X}, \mathcal{Y}} [\ell(f(X), Y)]}_{R(f)}\] <p>Here, $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ is the loss function, which measures the error between the prediction $f(X)$ and the true label $Y$. The term $R(f)$ is referred to as the risk of $f$. In simpler terms, the goal is to find a function that minimizes the average prediction error over the distribution $\mathcal{X}\times\mathcal{Y}$. However, solving this problem in practice presents two main challenges:</p> <ol> <li><strong>Optimization Challenge</strong>: It’s difficult to optimize over the space of all measurable functions $\mathcal{F}$.</li> <li><strong>Evaluation Challenge</strong>: Even evaluating the objective $R(f)$ requires performing an integration over a high-dimensional space, which is computationally challenging.</li> </ol> <p>The first challenge is often addressed by restricting our search to a parameterized family of functions ${f_\theta : \theta \in \Theta}$. This allows us to use optimization algorithms like gradient descent to find the optimal parameters $\theta^*$. The second challenge is trickier: Instead of directly optimizing the expectation, we approximate it using sample averages. This approach leads to the empirical risk minimization problem:</p> \[\hat{f}=\text{argmin}_{f\in \mathcal{F}} \underbrace{\frac{1}{n}\sum_{i=1}^n \ell(f(X_i), Y_i)}_{\hat{R}_n(f)}\] <p>Here, $\mathcal{D}={(X_i, Y_i)}_{i \in [n]}$, often called the training set, consists of i.i.d. samples from the distribution $\mathcal{X} \times \mathcal{Y}$. The term $\hat{R}_n(f)$ is the empirical risk, which is an approximation of the true risk $R(f)$ using the training data. <br/></p> <p>Since $\hat{f}$ is the minimizer of the empirical risk over the training dataset $\mathcal{D}$, we expect $\hat{f}$ to achieve low loss on $\mathcal{D}$. But this is not what we want: We want $\hat{f}$ to achieve low loss (in expectation) over the entire distribution $\mathcal{X}\times \mathcal{Y}$. We can quantify this by measuring the excess risk of $\hat{f}$:</p> \[\text{Excess}(\hat{f}) = R(\hat{f}) - R(f^*)\] <p>In other words, we compare the true risk of $\hat{f}$ with the optimal risk achieved by $f^*$. The hope is that, given some function class $\mathcal{F}$, we will be able to upper bound the excess risk, providing insights into the generalization capabilities of function estimators within $\mathcal{F}$.</p> <h3 id="decomposition-of-excess-risk">Decomposition of Excess Risk</h3> <p>To upper bound the excess risk, we can decompose it into three terms:</p> \[R(\hat{f}) - R(f^*) = \underbrace{R(\hat{f}) - \hat{R}_n(\hat{f})}_{(1)} + \underbrace{\hat{R}_n(\hat{f}) - \hat{R}_n(f)}_{(2)} + \underbrace{\hat{R}_n(f) - R(f^*)}_{(3)}\] <p>Since $\hat{f}$ is the minimizer of the empirical risk $\hat{R}_n$, the second term is non-positive. Therefore, our task reduces to bounding the first and third terms.</p> <ul> <li>The third term can usually be bounded using concentration inequalities.</li> <li>The first term, however, is more problematic since the i.i.d. assumption required for applying concentration inequalities is violated. Despite this, we proceed by applying a uniform bound:</li> </ul> \[\begin{align*} R(\hat{f}) - R(f^*) &amp;\leq \underbrace{R(\hat{f}) - \hat{R}_n(\hat{f})}_{(1)} + \underbrace{\hat{R}_n(f) - R(f^*)}_{(3)}\\ &amp;\leq 2\sup_{f\in \mathcal{F}}|\hat{R}_n(f)-R(f)| \end{align*}\] <p>We will discuss methods for bounding this uniform bound in the upcoming blog posts!</p>]]></content><author><name></name></author><category term="blog"/><category term="optimization"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Radamacher Complexity</title><link href="https://raymond112514.github.io/blog/2025/radamacher-complexity/" rel="alternate" type="text/html" title="Radamacher Complexity"/><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><id>https://raymond112514.github.io/blog/2025/radamacher-complexity</id><content type="html" xml:base="https://raymond112514.github.io/blog/2025/radamacher-complexity/"><![CDATA[<style>.post{line-height:2.0}details{background-color:#fff0f9;padding:1em;margin:1em 0;border-radius:4px}details summary{cursor:pointer;font-weight:normal;color:#9c27b0;list-style:none}details summary::before{content:"▼";color:#9c27b0;margin-right:8px}details[open] summary::before{content:"▼"}details:not([open]) summary::before{content:"▶"}details code{background-color:#fff0f9;padding:2px 4px;border-radius:3px}</style> <p>Note: Still in progress</p> <h3 id="bounding-the-excess-risk">Bounding the Excess Risk</h3> <p>Recall that the excess risk of the function class $\mathcal{F}$ can be bounded as follows</p> \[\begin{align*} R(\hat{f}) - R(f^*) &amp;\leq 2\cdot \underbrace{\sup_{f\in \mathcal{F}}\lvert\hat{R}_n(f)-R(f)\rvert}_{\|\mathbb{P}_n-\mathbb{P}\|_{\mathcal{F}}} \end{align*}\] <p>To bound the excess risk, it suffices to control ${\left\lVert\mathbb{P}_n-\mathbb{P}\right\rVert} _{\mathcal{F}}$. We claim that if the function class $\mathcal{F}$ is $b$-uniformly bounded, then ${\left\lVert\mathbb{P}_n-\mathbb{P}\right\rVert} _{\mathcal{F}}$ is sharply concentrated around its mean. We will establish this using McDiarmid’s inequality by showing ${\left\lVert\mathbb{P}_n-\mathbb{P}\right\rVert} _{\mathcal{F}}$ satisfies the bounded differences property. <br/></p> <details><summary>McDiarmid’s inequality</summary> <p>We say that $f:\mathbb{R}^n \rightarrow \mathbb{R}$ satisfy bounded difference property if there are constants $c_i\in \mathbb{R}$ such that</p> \[|f(x_1, x_2, ..., x_i, ..., x_n) - f(x_1, x_2, ..., x_i', ..., x_n)| \leq c_i\] <p>For all $x_1, x_2,…, x_n$ and $x_i’$. If $f$ satisfies bounded difference property, then for any independent random variables $X_1, X_2,…, X_n$, we have</p> \[\mathbb{P}[|f(X_1, X_2,..., X_n) - \mathbb{E}[f(X_1, X_2,..., X_n)]|\geq \epsilon] \leq \exp\bigg(-\frac{2\epsilon^2}{\sum_{i=1}^n c_i^2}\bigg)\] </details> <p>To prove this, fix the samples and define:</p> \[h(x_1, x_2,..., x_n) = \lvert\hat{R}_n(f)-R(f)\rvert = \bigg|\frac{1}{n}\sum_{i=1}^n f(x_i) - \mathbb{E}[f(X)]\bigg|\] <p>Now, consider perturbing only the first sample coordinate from $x_1$ to $x_1’$. We have:</p> \[\begin{align*} h(x_1, x_2,..., x_n) - \sup_{f\in \mathcal{F}} h(x_1', x_2,..., x_n) &amp;\leq h(x_1, x_2,..., x_n) - h(x_1', x_2,..., x_n)\\ &amp;\leq \frac{1}{n}|f(x_1)-f(x_1')|\leq \frac{2b}{n} \end{align*}\] <p>Here, the second inequality follows from the reverse triangle inequality. Taking the supremum over all $f\in \mathcal{F}$, we conclude that ${\left\lVert\mathbb{P}_n-\mathbb{P}\right\rVert} _{\mathcal{F}}$ indeed satisfies a bounded difference condition with bound $2b/n$. Applying McDiarmid’s inequality, we obtain the tail bound:</p> \[\|\mathbb{P}_n-\mathbb{P}\|_{\mathcal{F}} \leq \mathbb{E}[\|\mathbb{P}_n-\mathbb{P}\|_{\mathcal{F}}] + \delta \hspace{8mm}\text{with probability}\;1-\exp(-\frac{nt^2}{2b^2})\] <p>This suggests that if we can obtain a bound on the expectation, we can achieve a probabilistic upper bound for the excess risk.</p> <h3 id="radamacher-complexity">Radamacher Complexity</h3> <p>In this section, we will show that the expectation is closely related to a measure of the “size” or “complexity” of the function class. We first use a technique called <strong>symmetrization</strong> to bound the expectation. Let</p> \[\{Y_i\}_{i \in [n]}\] <p>be another sequence of i.i.d. random variables sampled from $\mathcal{X}$. Then</p> \[\begin{align*} \mathbb{E}[\|\mathbb{P}_n - \mathbb{P}\|_{\mathcal{F}}] &amp;= \mathbb{E}_X\left[\sup_{f \in \mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^n \Big(f(X_i) - \mathbb{E}[f(X)]\Big)\right|\right] \\ &amp;= \mathbb{E}_X\left[\sup_{f \in \mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^n \Big(f(X_i) - \mathbb{E}[f(Y_i)]\Big)\right|\right] \\ &amp;\leq \mathbb{E}_{X, Y}\left[\sup_{f \in \mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^n \Big(f(X_i) - f(Y_i)\Big)\right|\right] \end{align*}\] <p>Next, let $\epsilon_i$ be i.i.d. Rademacher random variables. Note that $f(X_i)-f(Y_i)$ has the same distribution as $\epsilon_i\bigl(f(X_i)-f(Y_i)\bigr)$. Therefore,</p> \[\begin{align*} \mathbb{E}[\|\mathbb{P}_n - \mathbb{P}\|_{\mathcal{F}}] &amp;\leq \mathbb{E}_{X, Y, \epsilon}\left[\sup_{f \in \mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^n \epsilon_i\bigl(f(X_i) - f(Y_i)\bigr)\right|\right] \\ &amp;\leq 2\,\mathbb{E}\left[\sup_{f \in \mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^n \epsilon_i f(X_i)\right|\right] \end{align*}\] <p>We call the quantity</p> \[\mathcal{R}_n(\mathcal{F}) = \mathbb{E}_{X, \epsilon}\left[\sup_{f \in \mathcal{F}}\left|\frac{1}{n}\sum_{i=1}^n \epsilon_i f(X_i)\right|\right]\] <p>the Rademacher complexity of the function class. It can be thought of as a measure of the size of the function class. We can interpret the Rademacher complexity as follows: suppose we are given a sample $\lbrace X_i \rbrace _{i \in [n]}$, and for each sample we assign a label $\epsilon_i \in \lbrace \pm 1 \rbrace$ by flipping a fair coin. For any function $f \in \mathcal{F}$, we can measure how well it fits the random data by evaluating the inner product</p> \[\text{success rate} = \frac{1}{n}\sum_{i=1}^n \epsilon_i f(X_i),\] <p>which reflects whether $f$ outputs the same sign as the random label. A high Rademacher complexity implies that, given any dataset $\lbrace X_i\rbrace _{i \in [n]}$ and random labels $\lbrace \epsilon_i \rbrace _{i \in [n]}$, we can always find a function $f \in \mathcal{F}$ with a high success rate—that is, one that is effective at interpolating the data. Naturally, a function class with limited representation power (e.g., the set of all linear functions) should have lower Rademacher complexity than function classes with rich representation power (such as neural networks).</p> <p>What we’ve shown is that the excess risk is essentially controlled by the representation power of the function class, which makes sense since a smaller function class is less likely to interpolate or overfit the data and should generalize better.</p> <h3 id="bounding-the-radamacher-complexity">Bounding the Radamacher complexity</h3> <p>There are several techniques available to control the Rademacher complexity of a function class. In some cases, it’s possible to obtain bounds using standard inequalities such as Cauchy-Schwarz or Jensen’s inequality. More generally, we rely on structural properties of the function class to derive meaningful bounds.To proceed, we define the set of possible function value vectors on a sample:</p> \[\mathcal{F}(x_{1:n}) = \{(f(x_1), f(x_2), \ldots, f(x_n)) \mid f \in \mathcal{F}\}\] <p>And we define $\lvert \mathcal{F}\rvert$ to be size of largest possible set, namely</p> \[|\mathcal{F}|= |\sup_{x_{1:n}} \mathcal{F}(x_{1:n})|\] <p>The size or structure of this set plays a key role in controlling the complexity. Below, we summarize some common approaches depending on whether $\mathcal{F}(x_{1:n})$ is finite or infinite:</p> <ul> <li>If $\lvert\mathcal{F}(x_{1:n})\rvert &lt; \infty$ <ul> <li>Use <strong>maximal inequalities</strong> to directly bound the complexity.</li> <li>Apply combinatorial tools like the <strong>VC dimension</strong> to relate the richness of $\mathcal{F}$ to generalization guarantees.</li> </ul> </li> <li>If $\lvert\mathcal{F}(x_{1:n})\rvert = \infty$ <ul> <li>Use <strong>metric entropy methods</strong>, such as covering numbers or chaining arguments, to quantify the effective complexity of the function class.</li> </ul> </li> </ul> <h4 id="maximal-inequality-bounds">Maximal Inequality Bounds</h4> <p>When $\lvert\mathcal{F}(x_{1:n})\rvert &lt; \infty$, we can control the Rademacher complexity using maximal inequality. Let’s start with the definition:</p> \[\mathcal{R}_n(\mathcal{F}) = \mathbb{E}_{X, \epsilon}\left[\sup_{f \in \mathcal{F}} \left|\frac{1}{n}\sum_{i=1}^n \epsilon_i f(X_i)\right|\right]\] <p>We can rewrite the inner product in terms of the function value vectors:</p> \[= \mathbb{E}_X\left[\mathbb{E}_\epsilon\left[\sup_{v \in \mathcal{F}(X_{1:n})} \left|\frac{1}{n} \langle v, \epsilon \rangle\right| \,\bigg|\, X_{1:n}\right]\right]\] <p>Here, each $\langle v, \epsilon \rangle / n$ is a sum of independent Rademacher variables, and thus is sub-Gaussian. The sub-Gaussian parameter is given by:</p> \[\sigma_n = \sup_{v \in \mathcal{F}(X_{1:n})} \frac{1}{n} \|v\|_2 = \sup_{f \in \mathcal{F}} \frac{1}{n} \sqrt{\sum_{i=1}^n f(X_i)^2}\] <p>Applying maximal inequality, we see that the conditional expectation is bounded by:</p> \[\mathbb{E}_\epsilon\left[\sup_{v \in \mathcal{F}(X_{1:n})} \left|\frac{1}{n} \langle v, \epsilon \rangle\right| \,\bigg|\, X_{1:n}\right] \leq \sigma_n \sqrt{2\log \big(2|\mathcal{F}(X_{1:n})|\big)}\] <p>Taking expectation over $X$, we have:</p> \[\begin{align*} \mathcal{R}_n(\mathcal{F}) &amp;\leq \mathbb{E}_X\left[\sqrt{\frac{1}{n} \sum_{i=1}^n f(X_i)^2} \cdot \sqrt{\frac{2 \log \big(2|\mathcal{F}(X_{1:n})|\big)}{n}}\right]\\ &amp;\leq \mathbb{E}_X\left[\sqrt{\frac{1}{n} \sum_{i=1}^n f(X_i)^2}\right] \cdot \sqrt{\frac{2 \log \big(2|\mathcal{F}|\big)}{n}}\\ &amp;\lesssim \sqrt{\frac{2\log \big(2|\mathcal{F}|\big)}{n}} \end{align*}\]]]></content><author><name></name></author><category term="blog"/><category term="optimization"/><summary type="html"><![CDATA[]]></summary></entry></feed>
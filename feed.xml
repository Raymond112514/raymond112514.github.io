<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://raymond112514.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://raymond112514.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-25T17:32:42+00:00</updated><id>https://raymond112514.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Introduction to Nonlinear Optimization</title><link href="https://raymond112514.github.io/blog/2025/descent-methods/" rel="alternate" type="text/html" title="Introduction to Nonlinear Optimization"/><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><id>https://raymond112514.github.io/blog/2025/descent-methods</id><content type="html" xml:base="https://raymond112514.github.io/blog/2025/descent-methods/"><![CDATA[<style>.post{line-height:2.0}</style> <h3 id="problem-setup">Problem Setup</h3> <p>The problem setup for optimization problem is quite simple: given a function $f:\mathbb{R}^n\rightarrow \mathbb{R}$ and some constrained set $C$, we want to solve:</p> \[\min_{x\in C}f(x)\] <p>For the first few blogs, we will consider the simple case where $C=\mathbb{R}^n$. In this case, the problem is an unconstrained optimization problem. Generally speaking, we require $f$ to have continuous derivative up to second order. In some algorithms, though, only continuity up to first order is needed. We also assume that $f$ is in general nonconvex.</p> <h3 id="preliminaries">Preliminaries</h3> <p>We first state some basic results that will be used later. One of the most important ones is Taylor’s theorem, on a high level, it tells us that we can approximate $f$ with some degree $n$ polynomial with high order error term. <br/></p> <p><strong>Theorem (Taylor)</strong>: Suppose $f:\mathbb{R}^n\rightarrow \mathbb{R}$ has continuous derivate up to order $2$, then for all $x,p\in \mathbb{R}^n$, there exists $t\in (0,1)$ such that</p> \[f(x+p)=f(x)+\nabla f(x+tp)^T p\] \[f(x+p)=f(x)+\nabla f(x)^Tp + \frac{1}{2}p^T\nabla^2 f(x+tp)p\] <h3 id="descent-method-recipe">Descent Method: Recipe</h3> <p>One common algorithm used to solve optimization problem is descent method. On a high level,descent method works iteratively, starting with some randomly chosen point $x_0$, and then updates the estimate gradually, most commonly through updates of the form</p> \[x_{k+1}=x_k+\alpha_k p_k\] <p>Where $p_k$ is the direction to step to and $\alpha_k$ controls the step size. This gives us a sequence ${x_k}$ and we terminate until a local minima is found. Broadly speaking, we choose $p_k$ to be a descent direction (i.e. moving in this direction reduces the function value).</p>]]></content><author><name></name></author><category term="blog"/><category term="optimization"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Introduction to Statistical Learning Theory</title><link href="https://raymond112514.github.io/blog/2025/introduction-to-statistical-learning-theory/" rel="alternate" type="text/html" title="Introduction to Statistical Learning Theory"/><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><id>https://raymond112514.github.io/blog/2025/introduction-to-statistical-learning-theory</id><content type="html" xml:base="https://raymond112514.github.io/blog/2025/introduction-to-statistical-learning-theory/"><![CDATA[<style>.post{line-height:2.0}</style> <h3 id="empirical-risk-minimization">Empirical Risk Minimization</h3> <p>In a standard supervised learning setup, we are given two spaces, $\mathcal{X}$ and $\mathcal{Y}$, and the goal is to learn a function $f \in \mathcal{F}$ that predicts $y \in \mathcal{Y}$ given $x \in \mathcal{X}$. This is typically done by solving the following optimization problem:</p> \[f^*=\text{argmin}_{f\in \mathcal{F}} \underbrace{\mathbb{E}_{X, Y\sim \mathcal{X}, \mathcal{Y}} [\ell(f(X), Y)]}_{R(f)}\] <p>Here, $\ell: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$ is the loss function, which measures the error between the prediction $f(X)$ and the true label $Y$. The term $R(f)$ is referred to as the risk of $f$. In simpler terms, the goal is to find a function that minimizes the average prediction error over the distribution $\mathcal{X}\times\mathcal{Y}$. However, solving this problem in practice presents two main challenges:</p> <ol> <li><strong>Optimization Challenge</strong>: It’s difficult to optimize over the space of all measurable functions $\mathcal{F}$.</li> <li><strong>Evaluation Challenge</strong>: Even evaluating the objective $R(f)$ requires performing an integration over a high-dimensional space, which is computationally challenging.</li> </ol> <p>The first challenge is often addressed by restricting our search to a parameterized family of functions ${f_\theta : \theta \in \Theta}$. This allows us to use optimization algorithms like gradient descent to find the optimal parameters $\theta^*$. The second challenge is trickier: Instead of directly optimizing the expectation, we approximate it using sample averages. This approach leads to the empirical risk minimization problem:</p> \[\hat{f}=\text{argmin}_{f\in \mathcal{F}} \underbrace{\frac{1}{n}\sum_{i=1}^n \ell(f(X_i), Y_i)}_{\hat{R}_n(f)}\] <p>Here, $\mathcal{D}={(X_i, Y_i)}_{i \in [n]}$, often called the training set, consists of i.i.d. samples from the distribution $\mathcal{X} \times \mathcal{Y}$. The term $\hat{R}_n(f)$ is the empirical risk, which is an approximation of the true risk $R(f)$ using the training data. <br/></p> <p>Since $\hat{f}$ is the minimizer of the empirical risk over the training dataset $\mathcal{D}$, we expect $\hat{f}$ to achieve low loss on $\mathcal{D}$. But this is not what we want: We want $\hat{f}$ to achieve low loss (in expectation) over the entire distribution $\mathcal{X}\times \mathcal{Y}$. We can quantify this by measuring the excess risk of $\hat{f}$:</p> \[\text{Excess}(\hat{f}) = R(\hat{f}) - R(f^*)\] <p>In other words, we compare the true risk of $\hat{f}$ with the optimal risk achieved by $f^*$. The hope is that, given some function class $\mathcal{F}$, we will be able to upper bound the excess risk, providing insights into the generalization capabilities of function estimators within $\mathcal{F}$.</p> <h3 id="decomposition-of-excess-risk">Decomposition of Excess Risk</h3> <p>To upper bound the excess risk, we can decompose it into three terms:</p> \[R(\hat{f}) - R(f^*) = \underbrace{R(\hat{f}) - \hat{R}_n(\hat{f})}_{(1)} + \underbrace{\hat{R}_n(\hat{f}) - \hat{R}_n(f)}_{(2)} + \underbrace{\hat{R}_n(f) - R(f^*)}_{(3)}\] <p>Since $\hat{f}$ is the minimizer of the empirical risk $\hat{R}_n$, the second term is non-positive. Therefore, our task reduces to bounding the first and third terms.</p> <ul> <li>The third term can usually be bounded using concentration inequalities.</li> <li>The first term, however, is more problematic since the i.i.d. assumption required for applying concentration inequalities is violated. Despite this, we proceed by applying a uniform bound:</li> </ul> \[\begin{align*} R(\hat{f}) - R(f^*) &amp;\leq \underbrace{R(\hat{f}) - \hat{R}_n(\hat{f})}_{(1)} + \underbrace{\hat{R}_n(f) - R(f^*)}_{(3)}\\ &amp;\leq 2\sup_{f\in \mathcal{F}}|\hat{R}_n(f)-R(f)| \end{align*}\] <p>We will discuss methods for bounding this uniform bound in the upcoming blog posts!</p>]]></content><author><name></name></author><category term="blog"/><category term="optimization"/><summary type="html"><![CDATA[]]></summary></entry></feed>